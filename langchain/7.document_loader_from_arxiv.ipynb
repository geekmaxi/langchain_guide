{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从Arxiv加载论文并进行摘要\n",
    "Arxiv网站上一篇《Efficient Prompt Compression with Evaluator Heads for Long-Context Transformer Inference》英文论文，其论文编号为：2501.12959。示例尝试加载这篇论文，并对其内容进行中文摘要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from chat_model_client import get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 加载论文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(metadata={'Published': '2025-01-22', 'Title': 'Efficient Prompt Compression with Evaluator Heads for Long-Context Transformer Inference', 'Authors': 'Weizhi Fei, Xueyan Niu, Guoqing Xie, Yingqing Liu, Bo Bai, Wei Han', 'Summary': 'Although applications involving long-context inputs are crucial for the\\neffective utilization of large language models (LLMs), they also result in\\nincreased computational costs and reduced performance. To address this\\nchallenge, we propose an efficient, training-free prompt compression method\\nthat retains key information within compressed prompts. We identify specific\\nattention heads in transformer-based LLMs, which we designate as evaluator\\nheads, that are capable of selecting tokens in long inputs that are most\\nsignificant for inference. Building on this discovery, we develop EHPC, an\\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\\n\"skim through\" input prompts by leveraging only the first few layers with\\nevaluator heads during the pre-filling stage, subsequently passing only the\\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\\nresults across two mainstream benchmarks: prompt compression and long-context\\ninference acceleration. Consequently, it effectively reduces the complexity and\\ncosts associated with commercial API calls. We further demonstrate that EHPC\\nattains competitive results compared to key-value cache-based acceleration\\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\\nfor long-context tasks.'}, page_content='Efficient Prompt Compression with Evaluator Heads for\\nLong-Context Transformer Inference\\nWeizhi Fei1,2, Xueyan Niu∗2, Guoqing Xie3, Yingqing Liu3, Bo Bai2, and Wei Han2\\n1Department of Mathematical Sciences, Tsinghua University, Beijing, China\\n2Theory Lab, 2012 Labs, Huawei Technologies Co., Ltd.\\n3Architecture & Design, ICT Products & Solutions, Huawei Technologies Co., Ltd.\\nAbstract\\nAlthough applications involving long-context inputs are crucial for the effective utilization of\\nlarge language models (LLMs), they also result in increased computational costs and reduced per-\\nformance. To address this challenge, we propose an efficient, training-free prompt compression\\nmethod that retains key information within compressed prompts. We identify specific attention\\nheads in transformer-based LLMs, which we designate as evaluator heads, that are capable of se-\\nlecting tokens in long inputs that are most significant for inference. Building on this discovery,\\nwe develop EHPC, an Evaluator Head-based Prompt Compression method, which enables LLMs\\nto rapidly “skim through” input prompts by leveraging only the first few layers with evaluator\\nheads during the pre-filling stage, subsequently passing only the important tokens to the model\\nfor inference. EHPC achieves state-of-the-art results across two mainstream benchmarks: prompt\\ncompression and long-context inference acceleration. Consequently, it effectively reduces the com-\\nplexity and costs associated with commercial API calls. We further demonstrate that EHPC attains\\ncompetitive results compared to key-value cache-based acceleration methods, thereby highlighting\\nits potential to enhance the efficiency of LLMs for long-context tasks.\\n1\\nIntroduction\\nLarge language models (LLMs) have exhibited exceptional capabilities in a variety of real-world tasks\\nand applications, with an increasing need for processing long inputs in areas such as literary novels,\\nlegal documents, instruction manuals, and code documentation. Inference tasks that requires under-\\nstanding of long contexts, such as long document summarization (Zhang et al., 2024), reasoning (Fei\\net al., 2024a), and autonomous agents (Singh et al., 2024; Chen et al., 2024), are of particular im-\\nportance due to the high stakes in these scenarios. However, the deployment of LLMs is challenged\\nby the computational and memory demands inherent to transformer-based architectures, resulting in\\nincreased latency, particularly when processing lengthy input prompts.\\nTable 1:\\nOverall comparison of the proposed\\nmethod in terms of average performance and la-\\ntency on the LongBench dataset, under the con-\\nstraint of a compressed prompt length of 2048 to-\\nkens. For comprehensive results, please see Table 4\\nand Table 5.\\nMethod\\nPerformance Latency Training-free\\nLongLLMLingua\\n48.0\\n67.44\\n✓\\nLLMLingua\\n34.6\\n7.51\\n✓\\nLLMLingua-2\\n39.1\\n1.27\\nX\\nEHPC (ours)\\n49.6\\n0.88\\n✓\\nPrompt compression, which entails substitut-\\ning the input prompts provided to a language\\nmodel with more succinct versions, has surfaced\\nas a promising strategy for enhancing long-text\\nunderstanding and mitigating associated costs.\\nCurrent mainstream methods, such as Select-\\nContext (Li et al., 2023), LLMLingua (Jiang\\net al., 2023a) and LongLLMLingua (Jiang et al.,\\n2023b), typically rely on pre-trained LLMs, uti-\\nlizing the logits or perplexity of the prompts\\nto evict tokens deemed insignificant. These ap-\\nproaches often necessitate chunking long texts for\\nprocessing, leading to numerous repeated calls\\nof the LLM and consequently incurring consider-\\nable time complexity. More efficient compression\\n∗Correspondence to: niuxueyan3@huawei.com.\\n1\\narXiv:2501.12959v1  [cs.CL]  22 Jan 2025\\n[BOS] … The best thing … is eat … on a sunny day … When cigarettes first appeared, … my parents smoked  …\\n“Needle”: should be retained \\n“Haystack”: can be discarded\\nInstruction: Please answer the question based on the content of the book. Question: What is the best thing to do in San Francisco? \\nContent:\\nFigure 1: Visualization of attention scores from a single attention head during inference on the “Needle-\\nin-a-Haystack” long-text benchmark. This benchmark requires the LLM to follow instructions and\\nretrieve “needles” – specific pieces of information randomly inserted into a long text – to answer a\\ngiven question. The evaluator heads are identified as those that accurately locate the relevant facts,\\nthereby achieving high scores.\\ntechniques, such as LLMLingua-2 (Pan et al., 2024), generally demand the training of a secondary,\\nsmaller model on labeled datasets. While these methods reduce compression time, they also incur\\nsubstantial training expenses and may exhibit performance drop in out-of-distribution contexts when\\ncompared to the direct utilization of the LLM as a compressor. In this paper, we present an Evaluator\\nHead-based Prompt Compression method, dubbed EHPC, which is built upon the efficient pre-filling\\nstage of LLMs. Our method leverages the intrinsic attention mechanisms of the LLM, thus being\\nboth training-free and computationally efficient, achieving state-of-the-art (SoTA) performance across\\nmainstream benchmarks (see Table 1).\\nThe attention mechanism is pivotal in transformer-based LLMs, aggregating information from input\\nprompts via attention scores. In our EHPC method, we utilize high attention scores to identify and retain\\nsignificant tokens. This approach is feasible primarily because the attention scores of tokens in long\\ntexts are sparse, explained by the widely studied “attention sink” phenomenon (Xiao et al., 2023; Gu\\net al., 2024). This phenomenon is characterized by LLMs’ frequently assigning high attention weights\\nto the semantically inconsequential initial token, <BOS>. Additionally, much of the research (Zhang\\net al., 2023; Li et al., 2024; Ge et al., 2024a; Cai et al., 2024; Wu et al., 2024; Tang et al., 2024; Xiao\\net al., 2024; Fu et al., 2024) has focused on compressing the key-value (KV) cache by eliminating entries\\nwith low attention weights. Furthermore, EHPC can be efficient because the attention computations\\ninvolved during the pre-filling stage are highly parallelizable.\\nUnlike our prompt compression method, which retains important tokens based on the scores of\\nevaluator heads, most KV cache compression methods preserve the cache according to all heads.\\nRecent research (Wu et al., 2024; Tang et al., 2024; Xiao et al., 2024) has identified that certain layers\\nand attention heads play a more significant role in processing long contexts. We have pinpointed a\\nsubset of these attention heads, which we designate as evaluator heads, allowing LLMs to focus on\\nessential information for inference from any position within the input sequence.\\nTo identify these\\nevaluator heads, we design a pilot experiment using synthetic data (see Figure 1). We then conduct\\nextensive experiments to demonstrate the robustness of the evaluator heads and their applicability to\\npractical scenarios using real-world datasets. Subsequently, we applied the evaluator heads to develop\\na prompt compression approach EHPC, utilizing the scores given by these heads to select tokens for\\ninference. The proposed EHPC requires the local development of LLMs for prompt compression and\\noffers two application settings: Extended Model Inference (EMI) and Native Model Inference (NMI).\\nWe demonstrate its effectiveness through two important benchmarks: prompt compression and long\\ntext acceleration. When prompts compressed using EHPC are employed in commercial models, EHPC\\neffectively reduces API costs while enhancing the performance of API outputs. Compared to existing\\nprompt compression methods (Li et al., 2023; Jiang et al., 2023b,a), our approach achieves new state-\\nof-the-art (SoTA) performance and is more efficient, requiring less compression time. Moreover, when\\napplied to native models deployed locally, prompts compressed with EHPC accelerate long-text inference\\nby reducing the memory usage and achieve competitive results compared to the SoTA KV cache\\n2\\ncompression methods (Li et al., 2024; Zhang et al., 2023). Specifically, our contributions are as follows:\\n• We identify specific attention heads within transformer-based LLMs, which we designate as eval-\\nuator heads, that are capable of selecting tokens in long inputs that are significant for inference.\\n• We develop EHPC, an efficient prompt compression technique that enables LLMs to quickly “skim\\nthrough” input prompts by utilizing only the first few layers with the evaluator heads, and then\\npass only the important tokens to the model for inference.\\n• We demonstrate that EHPC has lower complexity compared to prior prompt compression method\\nand achieves a new SoTA on the prompt compression benchmarks over LongBench and ZeroScroll,\\neffectively reducing the API cost and memory usage of commercial LLMs.\\n• We empirically demonstrate that EHPC is capable of accelerating long-context understanding,\\nachieving competitive performance relative to KV cache compression methods. Notably, EHPC\\nimproves upon direct inference by up to 40% on the question-answering datasets.\\n2\\nRelated Work\\nTwo predominant approaches are utilized for accelerating LLMs: implicit methods that reduce the KV\\ncache, and explicit methods that decrease the number of tokens. We briefly review these methods with\\na focus on the explicit methods, which can be applied to black-box LLMs such as GPT-3.5-Turbo.\\n2.1\\nImplicit Token Reduction\\nThe key-value (KV) cache reduces redundant calculations and enhances decoding efficiency by storing\\nkey and value matrices from previous tokens (Liu et al., 2024b; Adnan et al., 2024). However, as the\\ninput length increases, the memory requirements of the KV cache grow, creating significant challenges\\nfor long-context processing. It was found that a small number of tokens account for the majority\\nof the value during the computation of attention scores, leading to the proposal of the H2O (Zhang\\net al., 2023) that retains only the KV cache for “heavy hitters”, which are tokens with high attention\\nscores. FastGen (Ge et al., 2024a) introduces a dual-phase adaptive KV compression strategy that\\nincludes four KV cache compression policies and dynamically evicts caches during generation based\\non optimal policies identified through profiling. SnapKV (Li et al., 2024) demonstrates that specific\\nprompt attention patterns can be identified through an observation window at the end of prompts,\\nand it compresses KV caches by selecting clustered attention scores via pooling operations. Wu et al.\\n(2024) experimentally investigate how transformer-based models retrieve relevant information from\\narbitrary locations within long contexts, identifying certain heads, termed retrieval heads, as crucial\\nin this process.\\nSubsequently, building upon the concept of retrieval heads, several head-wise KV\\ncache compression methods have been proposed (Tang et al., 2024; Xiao et al., 2024). These methods\\nspecifically preserve the KV cache of retrieval heads to maintain their functionality.\\n2.2\\nExplicit Prompt Compression\\nSemantic compression\\nWingate et al. (2022) use soft prompts to condense context, ensuring that\\nthe compressed prompts retain a significant amount of information. Chevalier et al. (2023) introduce\\nAutoCompressor, which adapts LLMs for compressing lengthy contexts into concise summary vectors.\\nSimilarly, research by Mu et al. (2023) and Ge et al. (2024b) has focused on learning gist tokens\\nto compress context through prefix-tuning (Li and Liang, 2021).\\nFei et al. (2024b) implement a\\nsummarization model to semantically compress input text through a divide-and-conquer strategy.\\nToken deletion\\nA widely adopted approach among explicit methods is the direct removal of tokens\\n(Jha et al., 2024; Shi et al., 2024). Selective-Context (Li et al., 2023) utilize the logits of the language\\nmodel to calculate the mutual information of tokens, subsequently removing tokens based on this met-\\nric. LLMLingua (Jiang et al., 2023a) initially computes the perplexity of each token and then integrates\\na budget controller with a coarse-to-fine, token-level iterative compression algorithm. Expanding on\\nLLMLingua, LongLLMLingua (Jiang et al., 2023b) introduces the concept of conditional perplexity\\nto intensify the focus on key information in accordance with task-specific instructions, which achieves\\n3\\ngreat improvement over long text situation. LLMLingua-2 (Pan et al., 2024) represents a fast prompt\\ncompression method, as it employs a small classification model to predict the significance of each token\\nin the prompts. This classification model takes prompt compression as a token classification task and\\nis trained utilizing a compact transformer-based encoder on a labeled dataset.\\n3\\nMethodology\\nWe present our prompt compression method, EHPC, which is characterized by the identification and\\nutilization of evaluator heads. We find that in LLMs, certain attention heads, which we designate as\\nevaluator heads, can be utilized to rapidly determine which tokens can be omitted during the pre-filling\\nstage. Background information on the basic implementation of the multi-head attention mechanism in\\nLLMs, with a focus on the enhanced efficiency of the pre-filling stage through the application of KV\\ncache, is provided in Appendix A.\\n3.1\\nPrompt Compression Strategy\\nWe represent the input prompt as a sequence of tokens, x = (x1, x2, . . . , xN), where N = |x| denotes\\nthe sequence length. Let f be a language model. The objective of prompt compression is to identify\\na shorter sequence ˆx to replace the original sequence x, which can be mathematically formulated\\nas minˆx D(f(·|x), f(·|ˆx)), s.t. |ˆx| ≤|x|, where f(·|ˆx) represents the conditional distribution of the\\nlanguage model over the input prompts, and D is a divergence measuring the difference between the\\ndistributions.\\nAnalogous to the way human readers often skip words during speed reading, EHPC employs a token\\ndeletion strategy as in (Li et al., 2023; Jiang et al., 2023a) and compresses the prompt by dropping\\nnon-essential tokens directly.\\nIn contrast to generating new context through summarization\\n(Fei\\net al., 2024b), the token deletion strategy effectively simplifies the problem by narrowing the search\\nspace for prompt optimization. The token-level dropping strategy can be seamlessly integrated at the\\nsentence/paragraph level (Liskavets et al., 2024).\\n3.2\\nEvaluator Heads\\nWe seek to retain important tokens according to the attention scores. As detailed in Appendix A,\\neach attention head conducts a weighted average over preceding tokens, wherein tokens assigned lower\\nattention weights contribute less to the information processed by the attention heads. Empirical studies\\nhave demonstrated that the contribution of attention heads to the capacity for handling long contexts\\nin LLMs is not equally important (Wu et al., 2024; Tang et al., 2024). Specifically, certain retrieval\\nheads are essential and must be maintained during KV cache compression, as their removal would\\nsignificantly impair the LLMs’ ability to manage long contexts. We posit that specialized heads, which\\nwe designate as evaluator heads, play a pivotal role in assessing the significance of input prompts, and\\nthese evaluator heads alone are sufficient for evaluating the tokens.\\nWe conducted experiments based on synthetic data with known evidence to explore and verify the\\nexistence of the evaluator heads and that they can effectively identify the crucial information in the\\ninput prompts. For a transformer model f with L layers and H heads in each layer, we define the\\nevaluator heads as the set Cf = {(l1, h1), · · · , (lm, hm)}, where 1 ≤li ≤lj ≤L and 1 ≤hi ≤hj ≤H\\nfor i ≤j. These specialized heads are identified, and their attention scores are employed to give the\\nfinal score representing the utility of each token within the input prompts. We empirical investigate\\nproperties of evaluator heads, including the existence, generalizability, and robustness, in Section 4.\\nThe primary distinction between the evaluator heads we have defined and the retrieval heads dis-\\ncussed in Section 2.1 lies in their respective functions: evaluator heads are designed to assess the\\nsignificance of tokens within input prompts, while retrieval heads are intended to maintain the essen-\\ntial KV cache. Although both types of heads aim to identify the most salient components among all\\nattention heads through a data-driven approach, evaluator heads are deemed sufficient for their pur-\\npose, whereas retrieval heads are necessary for preserving the integrity of the KV cache. Furthermore,\\nevaluator heads operate explicitly on the tokens, in contrast to the retrieval heads, which function\\nimplicitly on the KV cache.\\n4\\nFigure 2: Heatmap of evidence scores for three different LLMs in the pilot experiment, illustrating\\nscores across layers and heads, with heads re-ranked in descending order for clarity.\\n3.3\\nPilot Experiments for Detecting Evaluator Heads\\nWe design a pilot experiment to identify the evaluator heads. By inputting a long prompt containing\\nknown evidence, we computed the corresponding scores for the evidence to pinpoint the evaluator\\nheads.\\nSpecifically, let ˆx = (x1, x2, . . . , xN) represent the long input sequence of N tokens, and let the\\nrelevant evidence e be denoted as the sub-sequence e = (xIe) of ˆx, where Ie ⊂[N] indicates the indices\\nof the evidence. Let Ahl ∈RH×L denote the matrix of attention scores for layer l and attention head\\nh using an LLM f. We then extract the last row of the attention matrix, ahl = Ahl[N, :] ∈RN, to\\nrepresent the scores for the importance of each token, as these scores directly influence the computation\\nof the final hidden state. To assess whether the heads are focusing on relevant information, we compute\\nthe accumulated score of the evidence as\\nˆahl =\\nX\\nj∈Ie\\nahl[j].\\n(1)\\nAs a practical example, we utilized the “Needle-in-a-Haystack” benchmark (Kamradt, 2024), a well-\\nestablished long-context retrieval benchmark, to demonstrate our pilot experiments (as illustrated in\\nFigure 1). Let ˆx be the synthesized long context, and let e represent the “needle” (evidence) inserted\\nat a specific position for f to identify. We recorded the accumulated score of the evidence ˆahl for\\neach head. We then averaged the accumulated scores of the evidence to generate an evidence score\\nmatrix S ∈RH×L, which we used to identify the evaluator heads. Visualizations of the evidence score\\nmatrices are presented in Figure 2 for several example open-source LLMs. Finally, we selected the\\nlayer with the highest score and identified the top-k heads from this layer as the evaluator heads. So\\nCf = arg max\\n|Λ|≤k\\n∥eΛS∥F\\ns.t.\\n(i, j) ̸∈λ, i = max\\n1≤l≤L(S · 1L×1)l, ∀j,\\nwhere ∥· ∥F is the Frobenius norm, eΛ = (eij) is the incidence matrix such that eij = 0 if (i, j) ̸∈Λ,\\nand eij = 1 if (i, j) ∈Λ.\\n3.4\\nPrompt Compression Using Evaluator Heads\\nOur prompt compression strategy selects salient tokens based on their averaged attention scores over\\nthe identified evaluator heads. Given a transformer-based language model f and the evaluator heads\\nCf identified through the pilot experiment, for a long input prompt x, we utilize the attention scores\\nfrom Cf to compute the utility scores s ∈RN for the input during the pre-filling stage according to\\ns =\\nX\\n(lj,hj)∈Cf\\nPool(\\nX\\nNr≤i≤N\\nAlj,hj[i, :]\\nNo\\n, r),\\n(2)\\n5\\nwhere Pool(·, ·) denotes a pooling operation, and r represents the kernel size. Subsequently, we employ\\nthe scores s to remove non-essential tokens, constructing the compressed prompt from the retained\\ntokens in their original order. Although the compressed prompt retains its natural language form, it\\nmay lack certain contextual elements. To mitigate this, we adopt a pooling operation, as described\\nin (Li et al., 2024), to group neighboring tokens with similar scores, thereby generating a continuous\\nsequence of compressed tokens that enhances readability.\\nWhile our prompt compression strategy necessitates the processing of prompts by an LLM, it\\nleverages the computationally efficient pre-filling stage, resulting in a relatively short compression\\ntime.\\nAs depicted in Figure 3, the proposed method reduces the number of tokens, thereby also\\nreducing the pre-filling latency during inference. The compressed prompts obtained are transferable\\nand can be applied to API-based black-box large models. The application of the compressed prompt\\ngives rise to two scenarios: Extended Model Inference (EMI) and Native Model Inference (NMI). EMI\\ninvolves using a different language model to infer the compressed prompt. For instance, when applied\\nto an API-based commercial model, this approach can reduce API latency and costs, as the API cost\\nis linearly related to the input prompt length. NMI, in contrast, utilizes the same language model to\\ninfer the compressed prompt. In this case, our method can decrease computational memory usage and\\ncosts, akin to the KV cache compression method. In practice, the transformer-based model f used for\\ncompressing prompts should exhibit robust long-context capabilities, and smaller models are preferred\\nfor efficient deployment, such as Llama-3.1-8B, with a context length of 128k.\\n3.5\\nComplexity\\nFigure 3: Illustration of the proposed method. (a)\\nLLM inference comprises two stages: the pre-filling\\nstage and the decoding stage. (b) The proposed\\nprompt compression approach leverages the effi-\\nciency of the pre-filling stage, thereby reducing\\ninference latency for both stages during inference\\nwith compressed context.\\nWe only discuss the complexity of NMI, as EMI\\nfollows a similar line of reasoning. Consider an\\nLLM f with L layers, H attention heads per\\nlayer, and a hidden dimension d = dkH. Sup-\\npose the model f is given an input prompt of N\\ntokens, and generates t new tokens. In the pre-\\nfilling stage, each head computes Softmax( QKT\\n√dk )\\nwith Q, K ∈RN×dk, resulting in a complexity of\\nO(dkN 2). Thus the total complexity for the pre-\\nfilling stage is O(LHdkN 2). During the decoding\\nstage, the model generates t tokens based on the\\npre-filled K, V ∈RN×dk, and the total complex-\\nity is O(LHdk(tN + t2\\n2 )), with details provided\\nin Appendix C.\\nIn the NMI setting, where f is used for both\\ncompression and inference, our method involves\\ntwo pre-filling stages and one decoding stage, as\\nillustrated in Figure 3. Let κ1 = L/ max1≤l≤L(S·\\n1L×1)l. The first pre-filling stage utilizes only L/κ1 layers to compress prompts, resulting in a complex-\\nity of O(LHdkN 2/κ1). Suppose the compression rate is κ2, then the second pre-filling stage processes\\nonly N/κ2 tokens, and the complexity is O( LHdkN2\\nκ2\\n2\\n). Thus the total complexity of combined pre-filling\\nstages is O(LHdkN 2( 1\\nκ1 +\\n1\\nκ2\\n2 )). Therefore, when κ1, κ2 ≥2, which is often the case, the pre-filling\\nwith our compressing method have lower complexity since ( 1\\nκ1 + 1\\nκ2\\n2 ) ≤3\\n4. The decoding stage involves\\nonly N/κ2 tokens, and the complexity is O(LHd(t2 + tN\\n2κ2 )), which is naturally lower than original\\ncomplexity since κ2 ≥1.\\n4\\nProperties of the Evaluator Heads\\nWe undertake a comprehensive investigation of the evaluator heads and aim to address the following\\nresearch questions:\\n(RQ1) Existence: Do evaluator heads exist significantly across LLMs? Can these heads be reliably\\nidentified through a pilot experiment utilizing synthetic data?\\n6\\n(RQ2) Practicality/Generalizability: Can the evaluator heads identified through synthetic data\\nbe effectively applied to real-world long-text benchmarks?\\n(RQ3) Robustness: Are evaluator heads task-oriented, i.e., do they demonstrate robustness and\\nconsistency across various downstream tasks?\\nTo investigate (RQ1), (RQ2), and (RQ3), we conduct pilot experiments to identify evaluator heads\\nusing a synthetic benchmark in the “Needle-in-a-Haystack” style. Then, we evaluate these evaluator\\nheads using real-world benchmarks, including LongBench (Bai et al., 2024). Details regarding the\\nsynthetic and real-world benchmarks used in our study are provided in Appendix D.\\nExistence\\nTo substantiate the existence of these heads, we conducted pilot experiments as detailed\\nin Section 3.3, utilizing questing-answering (QA) data from the “Needle-in-a-Haystack” benchmark.\\nThis dataset involves the random insertion of a sentence into a variable-length context, followed by\\nquerying the LLM to retrieve the specific sentence from the long context. We assessed each head’s\\nability to identify key information by examining the accumulated attention scores over the evidence\\nsequence. The results, visualized in Figure 2, demonstrate that several heads in the middle layers are\\ninstrumental in identifying key information relevant to the QA task within long contexts. Furthermore,\\nthe distribution of these evaluator heads is sparse and predominantly concentrated in the middle layers.\\nTable 2: Performance comparison of two sets of\\nheads on three datasets from different tasks in the\\nLongBench.\\nMF-en\\nMusique\\nTREC\\nEvaluator heads\\n27.7\\n13.7\\n62.5\\nComparison heads\\n24.3\\n12.1\\n60.5\\nPracticality\\nNext, we assess the practicality of\\nthe evaluator heads identified in our pilot experi-\\nments by evaluating their performance on down-\\nstream tasks. We select four heads with the high-\\nest scores as evaluator heads from the layer with\\nthe highest cumulative score, and we also select\\nfour additional heads from the same layer for\\ncomparison. The performances of these two sets\\nof heads are evaluated across three downstream\\ntasks from the LongBench benchmark. The re-\\nsults are presented in Table 2. The findings indi-\\ncate that the evaluator heads identified outper-\\nform the other heads on real-world data, thereby demonstrating the practical utility of the evaluator\\nheads.\\nTable 3:\\nEvaluation of task-aware approach on\\nmulti-hop reasoning and code completion from cor-\\nresponding LongBench datasets.\\nResults are av-\\neraged over the complete data. “Evaluator heads\\n(QA)” refers to the heads identified using QA\\ndata from the pilot experiment, while “Task-aware\\nheads” refers to heads identified from generated\\ndata customized to the respective tasks.\\nMulti-hop\\nCode\\nEvaluator heads (QA)\\n16.40\\n47.86\\nTask-aware heads\\n16.87\\n47.72\\nRobustness\\nSubsequently, we assess the ro-\\nbustness of our approach on tasks distinct from\\nQA in the pilot experiment. To this end, we gen-\\nerate probe data for two additional tasks: code\\ncompletion and multi-hop variable tracking. De-\\ntails of the construction of probe data is provided\\nin Appendix F. The goal is to identify the cor-\\nresponding heads for these scenarios, in addition\\nto the simple QA data. We then evaluate three\\ntypes of heads on their corresponding reasoning\\nand coding tasks from the LongBench dataset.\\nThe results are presented in Table 3. Our find-\\nings indicate that the heads identified using cus-\\ntomized tasks do not enhance performance on\\ntheir respective downstream tasks significantly.\\nThis demonstrates that the evaluator heads are task-agnostic and that the heads identified from the\\nsimple QA data exhibit robustness in downstream applications.\\n5\\nExperimental Results\\nHaving established the existence, practicality, and robustness of the evaluator heads, we now exper-\\nimentally assess the effectiveness and efficiency of EHPC in tasks aiming at reducing the API costs of\\n7\\nTable 4: Performance of various prompt compression methods under different compressed length con-\\nstraints on LongBench and ZeroSCROLLS. Higher values indicate better performance. The best scores\\nare highlighted in boldfaced.\\nMethods\\nLongBench\\nZeroSCROLLS\\nSingleDoc MultiDoc Summ. FewShot Synth. Code Avg. # Tokens\\nκ2\\nAvg. # Tokens\\nκ2\\nOriginal Prompt\\n39.7\\n38.7\\n26.5\\n67.0\\n37.8\\n54.2\\n44.0\\n10,295\\n-\\n32.5\\n9,788\\n-\\nZero-shot\\n15.6\\n31.3\\n15.6\\n40.7\\n1.6\\n36.2\\n23.5\\n214\\n48× 10.8\\n32\\n306×\\n3,000 tokens constraint\\nRetrieval-based Methods\\nBM25\\n32.3\\n34.3\\n25.3\\n57.9\\n45.1\\n48.9\\n40.6\\n3,417\\n3×\\n19.8\\n3,379\\n3×\\nSBERT\\n35.3\\n37.4\\n26.7\\n63.4\\n51.0\\n34.5\\n41.4\\n3,399\\n3×\\n24.0\\n3,340\\n3×\\nOpenAI\\n34.5\\n38.6\\n26.8\\n63.4\\n49.6\\n37.6\\n41.7\\n3,421\\n3×\\n22.4\\n3,362\\n3×\\nCompression-based Methods\\nSelective-Context\\n23.3\\n39.2\\n25.0\\n23.8\\n27.5\\n53.1\\n32.0\\n3,328\\n3×\\n20.7\\n3,460\\n3×\\nLLMLingua\\n31.8\\n37.5\\n26.2\\n67.2\\n8.3\\n53.2\\n37.4\\n3,421\\n3×\\n30.7\\n3,366\\n3×\\nLLMLingua-2\\n35.5\\n38.7\\n26.3\\n69.6\\n21.4\\n62.8\\n42.2\\n3,392\\n3×\\n33.5\\n3,206\\n3×\\nLongLLMLingua\\n40.7\\n46.2\\n27.2\\n70.6\\n53.0\\n55.2\\n48.8\\n3,283\\n3×\\n32.8\\n3,412\\n3×\\nEHPC (EMI)\\n44.2\\n49.1\\n25.1\\n68.8\\n54.0\\n63.0 49.7\\n2,892\\n3×\\n36.7\\n3,005\\n3×\\n2,000 tokens constraint\\nRetrieval-based Methods\\nBM25\\n30.1\\n29.4\\n21.2\\n19.5\\n12.4\\n29.1\\n23.6\\n1,985\\n5×\\n20.1\\n1,799\\n5×\\nSBERT\\n33.8\\n35.9\\n25.9\\n23.5\\n18.0\\n17.8\\n25.8\\n1,947\\n5×\\n20.5\\n1,773\\n6×\\nOpenAI\\n34.3\\n36.3\\n24.7\\n32.4\\n26.3\\n24.8\\n29.8\\n1,991\\n5×\\n20.6\\n1,784\\n5×\\nCompression-based Methods\\nSelective-Context\\n16.2\\n34.8\\n24.4\\n15.7\\n8.4\\n49.2\\n24.8\\n1,925\\n5×\\n19.4\\n1,865\\n5×\\nLLMLingua\\n22.4\\n32.1\\n24.5\\n61.2\\n10.4\\n56.8\\n34.6\\n1,950\\n5×\\n27.2\\n1,862\\n5×\\nLLMLingua-2\\n29.8\\n33.1\\n25.3\\n66.4\\n21.3\\n58.9\\n39.1\\n1,954\\n5×\\n33.4\\n1,898\\n5×\\nLongLLMLingua\\n39.0\\n42.2\\n27.4\\n69.3\\n53.8\\n56.6\\n48.0\\n1,809\\n6×\\n32.5\\n1,753\\n6×\\nEHPC (EMI)\\n44.5\\n50.7\\n24.8\\n68.9\\n51.5\\n61.9 49.6\\n2,004\\n5×\\n34.6\\n2,041\\n5×\\ncommercial models and accelerating long-context inference in LLMs. We also compare the efficiency\\nof our prompt compression method to other acceleration methods under the same memory usage in\\nSection 5.2.\\n5.1\\nPrompt Compression Benchmark\\nWe evaluate the prompt compression benchmark on LongBench and ZeroSCROLLS, as established by\\nJiang et al. (2023b), with the objective of enhancing the quality of compressed prompts for commercial\\nmodels. Prompt compression is essential for mitigating costs associated with commercial LLMs, given\\nthat API fees are often proportional to prompt length. We compare our EMI setting, which employs a\\nlocal LLM for prompt compression and another model for inferring the compressed prompts, against\\nthe following baselines.\\nBaselines\\nWe compare our method with Retrieval Augmented Generation (RAG) and other SoTA\\nprompt compression methods. The retrieval models considered include BM25, SentenceBERT (Reimers\\nand Gurevych, 2019), and OpenAI Embedding. For prompt compression methods, we evaluate against\\nSelective-Context (Li et al., 2023), LLMLingua (Jiang et al., 2023a), LongLLMLingua (Jiang et al.,\\n2023b), and LLMLingua-2 (Pan et al., 2024) as baselines. Detailed descriptions of these prompt com-\\npression baselines are provided in Appendix E. LongLLMLingua extends LLMLingua by incorporating\\ntask-specific information to enhance long-context compression, while LLMLingua-2 improves efficiency\\nthrough the use of a smaller model.\\nImplementation Detail\\nTo ensure reproducibility, we employ greedy decoding and set the tem-\\nperature to 0 in all experiments. For prompt compression, we utilize the Llama-3.1-8B model1. For\\ninference, we use GPT3.5-turbo, accessible via OpenAI. Additional details of hyper-parameters are\\nprovided in Appendix B.\\n1available at: https://huggingface.co/meta-llama/Meta-Llama-3-8B\\n8\\nTable 5: The averaged time (in seconds) for different methods applied to a subset of LongBench,\\ntargeting compression to 2,048 tokens. The subset contains 10 examples from RepoBench-P, with an\\naverage of 14,354 tokens. Each example was repeated 5 times to reduce randomness. Specifically,\\nChatGPT-3.5-Turbo (all tokens) serves as the baseline for inference of the original prompts.\\nMethod\\nLatency\\nCompression\\ntime\\nInference time\\n(2048 tokens)\\nTotal\\nLLMLingua-2\\n7.51\\n1.09\\n8.60\\nLongLLMLingua\\n67.44\\n1.31\\n68.74\\nLLMLingua-2\\n1.27\\n1.15\\n2.37\\nEHPC (ours)\\n0.88\\n1.11\\n1.99\\nChatGPT-3.5-Turbo (all tokens)\\n2.16\\nResults\\nThe results of the prompt compression benchmarks are presented in Table 4. Consistent\\nwith previous research (Jiang et al., 2023b), we report the averaged results for ZeroSCROLLS and\\nLongBench, as well as the average performance for the sub-tasks on LongBench, with target com-\\npressed prompt lengths of 2000 and 3000 tokens. Table 4 illustrates that our method achieves superior\\nperformance on average across both benchmarks under the length constraints. The results show that\\nEHPC surpasses the original prompts and significantly enhances the accuracies of the QA tasks, indi-\\ncating its effectiveness in retrieval tasks by alleviating the disturbance of noisy context. Notably, our\\nmodel performs well on code tasks in the LongBench, which can be attributed to the identification\\nof the key tokens involved in the inference of coding tasks. Additionally, the performance of EHPC on\\nsummarization and few-shot learning tasks is competitive. Overall, our compression method achieves\\nhigh-quality compression, even outperforming the original prompts on specific tasks.\\nCompression Latency\\nWe evaluate the running times of various methods using a subset of Long-\\nBench, including direct inference, LLMLingua-2, LongLLMLingua, and our proposed approach. The\\nresults are presented in Table 5. Each prompt compression method reduces the average prompt length\\nfrom 14,354 tokens to 2,048 tokens. The experiments were conducted on a GPU with 40GB of VRAM.\\nFor a fair comparison, we utilized the same language model, Llama-3.1-8B, for all methods except\\nLLMLingua-2, which requires a specialized model. The results in Table 5 indicate that our method is\\nsignificantly faster than LongLLMLingua and also outperforms LLMLingua-2, which is known for its\\nefficiency. The lower latency of our compression strategy is attributed to its reliance on the efficient\\npre-filling stage.\\n5.2\\nAcceleration of LLM Inference\\nWe demonstrate that EHPC effectively reduces memory overhead and computation costs during long-\\ncontext inference of LLMs by compressing input prompts. By inferring over the k times compressed\\nprompt, the KV cache during pre-filling is reduced k times, also accelerating the decoding stage. In\\nthe NMI setting, where the same model is used for both compression and inference, the achieved\\nacceleration is comparable to that of KV cache compression methods with the same compression\\nratio. Therefore, we compare our method against the KV cache compression method on the long-text\\nacceleration benchmark of LongBench.\\nBaselines\\nWe consider acceleration frameworks that include KV cache compression and prompt\\ncompression as baselines. For KV cache compression methods, we select SoTA approaches, including\\nH2O (Zhang et al., 2023) and SnapKV (Li et al., 2024), which eliminate unnecessary KV caches based\\non attention scores. For the prompt compression method, we select GemFilter (Shi et al., 2024), which\\nutilizes attention scores from early layers to compress prompts.\\nImplementation Detail\\nWe utilize two popular long-context models: Llama-3.1-8B and Phi-3.8B,\\nboth support a context window length of 128k. For each method, we set the target lengths to 1024\\n9\\nTable 6: Performance of different acceleration methods over various LLMs under the same KV cache\\nmemory usage. Higher values indicate better performance. The best scores are boldfaced.\\nMethod\\nSingle-Document QA\\nMulti-Document QA\\nSummarization\\nFew-shot Learning\\nSynthetic\\nCode\\nAverage\\nNrtvQA\\nQasper\\nMF-en\\nHotpotQA\\n2WikiMQA\\nMusique\\nGovReport\\nQMSum\\nMultiNews\\nTREC\\nTriviaQA\\nSAMSum\\nPCount\\nPRe\\nRepoBench-P\\nLCC\\nLLaMA 3.1 8B Instruct\\nAll KV\\n32.02\\n13.04\\n27.34\\n16.23\\n16.05\\n11.22\\n34.52\\n23.41\\n26.89\\n73.0\\n91.64\\n43.8\\n7.16\\n97.73\\n49.25\\n52.7\\n38.32\\nH2O-1024\\n21.98\\n10.96\\n23.17\\n16.06\\n15.16\\n10.15\\n30.76\\n23.75\\n26.32\\n68.0\\n90.68\\n42.54\\n7.40\\n71.34\\n51.6\\n46.4\\n34.77\\nSnapKV-1024\\n31.98 11.17\\n25.33\\n14.81\\n15.73\\n10.69\\n26.95\\n22.89\\n25.86\\n67.5\\n91.89\\n42.85\\n7.67 98.16\\n48.7\\n52.1\\n35.25\\nGemFilter-1024\\n20.71\\n11.00\\n29.28\\n19.12\\n17.01\\n13.01\\n30.37\\n21.75\\n25.17\\n63.0\\n90.70\\n42.50\\n7.15\\n92.22\\n35.0\\n38.5\\n34.50\\nEHPC (ours)-1024 22.98 13.02\\n27.41\\n20.64\\n16.97\\n13.99\\n29.49\\n24.15\\n25.24\\n68.0\\n86.52\\n41.86\\n5.79\\n97.21\\n42.8\\n48.9\\n35.23\\nH2O-2048\\n23.16\\n11.68\\n25.09\\n16.17\\n15.22\\n9.93\\n32.13\\n23.32\\n26.73\\n68.5\\n91.32\\n43.97\\n6.52\\n72.79\\n52.5\\n48.1\\n35.45\\nSnapKV-2048\\n31.45 11.94\\n26.24\\n15.73\\n16.03\\n11.66\\n29.64\\n23.24\\n26.44\\n69.5\\n91.48\\n42.68\\n7.21\\n98.03\\n49.5\\n52.6\\n35.80\\nGemFilter-2048\\n24.36\\n12.63\\n25.39\\n19.58\\n17.03\\n14.11\\n33.15\\n22.31\\n26.49\\n69.5\\n91.59\\n42.64\\n4.61 98.75\\n38.8\\n47.3\\n35.87\\nEHPC (ours)-2048 20.98 14.38\\n30.50\\n19.35\\n16.23\\n13.02\\n32.9\\n24.54\\n26.72\\n71.0\\n90.47\\n42.24\\n9.35 97.94\\n44.9\\n52.2\\n37.86\\nPhi 3.5 Mini 3.8B Instruct\\nAll KV\\n27.51\\n17.23\\n35.63\\n21.70\\n25.70\\n11.68\\n34.14\\n23.17\\n24.95\\n71.5\\n87.37\\n13.08\\n7.17\\n83.85\\n46.0\\n46.2\\n34.62\\nH2O-1024\\n18.25\\n12.97\\n29.69\\n20.75\\n20.90\\n9.90\\n32.02\\n21.69\\n24.70\\n67.5\\n85.45\\n20.16\\n1.37\\n46.80\\n46.6\\n43.3\\n31.38\\nSnapKV-1024\\n24.31 16.03\\n34.93\\n20.72\\n26.02\\n13.74\\n28.27\\n22.03\\n24.02\\n67.5\\n87.71\\n14.57\\n6.08 85.60\\n44.0\\n47.1\\n33.68\\nGemFilter-1024\\n16.57\\n18.29\\n35.91\\n24.22\\n26.10\\n9.70\\n30.29\\n18.96\\n23.64\\n64.5\\n85.85\\n23.02\\n0.20\\n81.12\\n39.0\\n40.7\\n32.74\\nEHPC (ours)-1024 23.91 32.22\\n45.36\\n44.97\\n32.79\\n20.27\\n31.90\\n22.19\\n23.77\\n68.5\\n85.72\\n36.69\\n1.80\\n79.08\\n38.7\\n41.6\\n39.22\\nH2O-2048\\n18.26\\n14.05\\n32.4\\n20.03\\n22.51\\n10.30\\n32.86\\n21.43\\n24.90\\n67.2\\n86.44\\n19.65\\n1.43\\n46.96\\n47.89\\n44.71\\n31.94\\nSnapKV-2048\\n26.41\\n16.59\\n36.99\\n21.80\\n26.07\\n12.57\\n30.88\\n22.37\\n24.51\\n69.5\\n87.54\\n13.13\\n6.57 83.92\\n45.30\\n46.70\\n34.20\\nGemFilter-2048\\n19.63\\n14.84\\n35.99\\n21.38\\n19.72\\n10.13\\n32.39\\n21.24\\n24.71\\n65.0\\n86.49\\n20.47\\n2.17\\n69.50\\n46.30\\n48.10\\n31.69\\nEHPC (ours)-2048 24.80 39.27\\n39.78\\n27.06\\n25.22\\n14.05\\n33.26\\n23.52\\n24.48\\n68.0\\n87.66\\n37.77\\n2.54\\n63.00\\n47.31\\n45.44\\n36.46\\nand 2048 tokens for prompt compression and KV cache compression respectively.\\nResults\\nIn Table 6, we present the results of long-text inference acceleration under KV cache con-\\nstraints of 1024 and 2048 tokens using various methods. EHPC achieves the best average performance\\ncompared to the baselines across different compression rates. Relative to KV cache-based methods,\\nour prompt compression method performs particularly well on specific tasks such as QA, even out-\\nperforming the results obtained using the full KV cache. This improvement is particularly significant\\nwhen applying our method to the Phi-3.8B model, enhancing its performance on direct inference by an\\naverage of up to 40% averaged on QA tasks, including Single-Document QA and Multi-Document QA.\\nHowever, prompt compression exhibits a decline in performance on code-related tasks and few-shot\\nlearning tasks. While the effectiveness of the KV cache compression method decreases gradually as\\nthe KV cache memory is reduced, it remains more advantageous than prompt compression for these\\ntasks. Overall, EHPC demonstrates competitive results compared to the KV cache compression method,\\nparticularly in QA tasks.\\n6\\nConclusion\\nIn this paper, we introduce EHPC, an efficient and effective prompt compression method that utilizes\\nevaluator heads to leverage attention scores from the pre-filling stage of transformer-based large lan-\\nguage models. Unlike previous speedup methods that rely on training specialized small models, our\\napproach is training-free and achieves state-of-the-art performance in prompt compression. Specifi-\\ncally, we empirically identify certain attention heads, designated as evaluator heads, that effectively\\nidentify important tokens in long inputs.\\nWe investigate their existence, practicality, and robust-\\nness using both synthetic and real-world data. Subsequently, we implement our evaluator head-based\\nprompt compression (EHPC ) method in two settings: native model inference and extended model infer-\\nence. We demonstrate the effectiveness of EHPC in these settings across two mainstream benchmarks,\\nhighlighting its potential to significantly reduce API costs for commercial use and accelerate long-text\\ninference.\\n10\\nReferences\\nMuhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant Nair, Ilya Soloveychik, and Pu-\\nrushotham Kamath. 2024. Keyformer: KV cache reduction through key tokens selection for efficient\\ngenerative inference. Proceedings of Machine Learning and Systems, 6:114–127.\\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao\\nLiu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024. LongBench: A bilingual,\\nmultitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of\\nthe Association for Computational Linguistics (Volume 1: Long Papers), pages 3119–3137, Bangkok,\\nThailand. Association for Computational Linguistics.\\nZefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong,\\nBaobao Chang, Junjie Hu, et al. 2024.\\nPyramidKV: Dynamic KV cache compression based on\\npyramidal information funneling. arXiv preprint arXiv:2406.02069.\\nYanan Chen, Ali Pesaranghader, Tanmana Sadhu, and Dong Hoon Yi. 2024.\\nCan we rely on\\nLLM agents to draft long-horizon plans? let’s take travelplanner as an example. arXiv preprint\\narXiv:2408.06318.\\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. Adapting language models\\nto compress contexts. ArXiv, abs/2305.14788.\\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. 2021. A dataset\\nof information-seeking questions and answers anchored in research papers. In Proceedings of the\\n2021 Conference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, pages 4599–4610.\\nAlexander Richard Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. 2019. Multi-news: A\\nlarge-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings\\nof the 57th Annual Meeting of the Association for Computational Linguistics, pages 1074–1084.\\nWeizhi Fei, Xueyan Niu, Guoqing Xie, Yanhua Zhang, Bo Bai, Lei Deng, and Wei Han. 2024a. Re-\\ntrieval meets reasoning: Dynamic in-context editing for long-text understanding. arXiv preprint\\narXiv:2406.12331.\\nWeizhi Fei, Xueyan Niu, Pingyi Zhou, Lu Hou, Bo Bai, Lei Deng, and Wei Han. 2024b. Extending\\ncontext window of large language models via semantic compression. In Findings of the Association for\\nComputational Linguistics: ACL 2024, pages 5169–5181. Association for Computational Linguistics.\\nQichen Fu, Minsik Cho, Thomas Merth, Sachin Mehta, Mohammad Rastegari, and Mahyar Najibi.\\n2024. Lazyllm: Dynamic token pruning for efficient long context LLM inference. arXiv preprint\\narXiv:2407.14057.\\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. 2024a. Model\\ntells you what to discard: Adaptive KV cache compression for LLMs. In The Twelfth International\\nConference on Learning Representations.\\nTao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. 2024b. In-context autoencoder\\nfor context compression in a large language model. In The Twelfth International Conference on\\nLearning Representations.\\nXiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, and Min\\nLin. 2024. When attention sink emerges in language models: An empirical view. arXiv preprint\\narXiv:2410.10781.\\nDaya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. 2023. Longcoder: A long-range\\npre-trained language model for code completion. In International Conference on Machine Learning.\\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing a multi-\\nhop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th In-\\nternational Conference on Computational Linguistics, pages 6609–6625, Barcelona, Spain (Online).\\nInternational Committee on Computational Linguistics.\\n11\\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris\\nGinsburg. 2024. RULER: What’s the real context size of your long-context language models?\\nIn\\nFirst Conference on Language Modeling.\\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient attentions\\nfor long document summarization. In Proceedings of the 2021 Conference of the North American\\nChapter of the Association for Computational Linguistics: Human Language Technologies, pages\\n1419–1436, Online. Association for Computational Linguistics.\\nSiddharth Jha, Lutfi Eren Erdogan, Sehoon Kim, Kurt Keutzer, and Amir Gholami. 2024. Character-\\nizing prompt compression methods for long context inference. arXiv preprint arXiv:2407.08892.\\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023a. LLMLingua: com-\\npressing prompts for accelerated inference of large language models. In Proceedings of the 2023\\nConference on Empirical Methods in Natural Language Processing, pages 13358–13376.\\nHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.\\n2023b. LongLLMLingua: accelerating and enhancing LLMs in long context scenarios via prompt\\ncompression. arXiv preprint arXiv:2310.06839.\\nGreg Kamradt. 2024. Needle In A Haystack - Pressure Testing LLMs. https://github.com/gkamradt/\\nLLMTest NeedleInAHaystack.\\nTom´aˇs Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G´abor Melis,\\nand Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. Transactions of\\nthe Association for Computational Linguistics, 6:317–328.\\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation.\\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and\\nthe 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),\\npages 4582–4597, Online. Association for Computational Linguistics.\\nXin Li and Dan Roth. 2002. Learning question classifiers. In COLING 2002: The 19th International\\nConference on Computational Linguistics.\\nYucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. 2023.\\nCompressing context to enhance\\ninference efficiency of large language models. arXiv preprint arXiv:2310.06201.\\nYuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai,\\nPatrick Lewis, and Deming Chen. 2024.\\nSnapKV: LLM knows what you are looking for before\\ngeneration. In The Thirty-eighth Annual Conference on Neural Information Processing Systems.\\nBarys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, and Shane Luke. 2024.\\nPrompt compression with context-aware sentence encoding for fast and improved LLM inference.\\narXiv preprint arXiv:2409.01227.\\nTianyang Liu, Canwen Xu, and Julian McAuley. 2024a. Repobench: Benchmarking repository-level\\ncode auto-completion systems.\\nZichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyril-\\nlidis, and Anshumali Shrivastava. 2024b. Scissorhands: Exploiting the persistence of importance\\nhypothesis for LLM KV cache compression at test time. Advances in Neural Information Processing\\nSystems, 36.\\nJesse Mu, Xiang Lisa Li, and Noah Goodman. 2023. Learning to compress prompts with gist tokens.\\nIn Thirty-seventh Conference on Neural Information Processing Systems.\\nXueyan Niu, Bo Bai, Lei Deng, and Wei Han. 2024. Beyond scaling laws: Understanding transformer\\nperformance with associative memory. arXiv preprint arXiv:2405.08707.\\nZhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor\\nR¨uhle, Yuqing Yang, Chin-Yew Lin, et al. 2024. LLMLingua-2: data distillation for efficient and\\nfaithful task-agnostic prompt compression. arXiv preprint arXiv:2403.12968.\\n12\\nNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using Siamese BERT-\\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Process-\\ning and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),\\npages 3982–3992, Hong Kong, China. Association for Computational Linguistics.\\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023. ZeroSCROLLS: A zero-\\nshot benchmark for long text understanding.\\nIn Findings of the Association for Computational\\nLinguistics: EMNLP 2023, pages 7977–7989, Singapore. Association for Computational Linguistics.\\nZhenmei Shi, Yifei Ming, Xuan-Phi Nguyen, Yingyu Liang, and Shafiq Joty. 2024. Discovering the\\ngems in early layers: Accelerating long-context LLMs with 1000x input token reduction.\\narXiv\\npreprint arXiv:2409.17422.\\nSimranjit Singh, Andreas Karatzas, Michael Fore, Iraklis Anagnostopoulos, and Dimitrios Stamoulis.\\n2024. An LLM-Tool compiler for fused parallel function calling. arXiv preprint arXiv:2405.17438.\\nHanlin Tang, Yang Lin, Jing Lin, Qingsen Han, Shikuan Hong, Yiwu Yao, and Gongyi Wang.\\n2024.\\nRazorattention: Efficient KV cache compression through retrieval heads.\\narXiv preprint\\narXiv:2407.15891.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022.\\nMuSiQue:\\nMultihop questions via single-hop question composition. Transactions of the Association for Com-\\nputational Linguistics, 10:539–554.\\nDavid Wingate, Mohammad Shoeybi, and Taylor Sorensen. 2022. Prompt compression and contrastive\\nconditioning for controllability and toxicity reduction in language models. In Findings of the Asso-\\nciation for Computational Linguistics: EMNLP 2022, pages 5621–5634, Abu Dhabi, United Arab\\nEmirates. Association for Computational Linguistics.\\nWenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, and Yao Fu. 2024. Retrieval head mecha-\\nnistically explains long-context factuality. arXiv preprint arXiv:2404.15574.\\nGuangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and\\nSong Han. 2024. Duoattention: Efficient long-context LLM inference with retrieval and streaming\\nheads. arXiv preprint arXiv:2410.10819.\\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. Efficient streaming\\nlanguage models with attention sinks. arXiv preprint arXiv:2309.17453.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and\\nChristopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question\\nanswering. In Conference on Empirical Methods in Natural Language Processing (EMNLP).\\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B\\nHashimoto. 2024.\\nBenchmarking large language models for news summarization.\\nTransactions\\nof the Association for Computational Linguistics, 12:39–57.\\nZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song,\\nYuandong Tian, Christopher R´e, Clark Barrett, Zhangyang ”Atlas” Wang, and Beidi Chen. 2023.\\nH2o: Heavy-hitter oracle for efficient generative inference of large language models. In Advances in\\nNeural Information Processing Systems, volume 36, pages 34661–34710. Curran Associates, Inc.\\nMing Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan, Asli\\nCelikyilmaz, Yang Liu, Xipeng Qiu, et al. 2021. Qmsum: A new benchmark for query-based multi-\\ndomain meeting summarization.\\nIn Proceedings of the 2021 Conference of the North American\\nChapter of the Association for Computational Linguistics: Human Language Technologies, pages\\n5905–5921.\\nYinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao\\nZhang. 2024. Distserve: Disaggregating prefill and decoding for goodput-optimized large language\\nmodel serving. arXiv preprint arXiv:2401.09670.\\n13\\nA\\nBackground\\nMulti-head Attention\\nTransformer-based models autoregressively predict the next token based\\non the τ preceding tokens according to\\nhl = hl−1 + δl + hl, ml = FFN(hl−1 + δl)\\nwhere hl, ml, δl ∈Rd are the hidden states of the l-th layer and FFN(·) denotes the feed forward\\nnetwork. Readers who are interested in more details and mathematical implications can refer to (Niu\\net al., 2024). Transformer models typically utilize multi-head attention, so that the hidden state δl at\\nlayer l is computed as\\nδl = W l\\na ConCat(ˆhl1, · · · , ˆhlH),\\nwhere H is the number of heads in each layer, and ˆhlh denotes the hidden representation of the h-th\\nhead at layer l. The attention operation in each attention head is\\nˆhlh = Softmax(Qlh(Klh)T\\n√dk\\n) · V lh,\\n(3)\\nwhere Qlh, Klh, V lh ∈Rτ×dk, and dk = d/H is the dimension of each head. As shown in Eq. (3), the\\nattention mechanism aggregates information and selects important tokens from input prompts. The\\nweights calculated by the dot product between queries and keys determine which tokens of the values\\nare considered within this attention block. The independent multi-head attention mechanism enables\\nthe model to capture information from previous tokens in multiple ways.\\nLLM Inference and KV Cache Compression\\nContemporary decoder-only LLMs generating new\\ntokens with a series of tokens as input involves two stages: the pre-filling stage and the decoding stage.\\nDuring the pre-filling stage, the model processes compute the intermediate states (keys and values in\\nattention operations), which is highly parallelized and computational efficient. In the decoding stage,\\nLLMs load the precomputed KV cache and generate each output token autoregressively through a\\nforward pass, which is slower due to its serial nature (Zhong et al., 2024).\\nWhen processing long text inputs, the size of the corresponding KV cache increases dramatically,\\nsignificantly raising both computational costs and time. To tackle this issue, KV cache compressing\\nmethods that eliminate the unnecessary KV caches have been proposed. Many KV cache compression\\nmethods are based on the attention weights to propose a policy to determine which tokens to retain\\nin memory. Let Alh = Softmax( Qlh(Klh)T\\n√dk\\n) represent the attention matrix. The policy first averages\\nthe last rows to represent the scores of input tokens, as follows:\\ns =\\nX\\nNr≤i≤N\\nA[i, :]\\nNo\\n,\\n(4)\\nwhere N denote the number of tokens in the prompt, No is the observed windows length and Nr is\\nthe length of remaining part, such that N = Nr + No. To further improve the contextual integrity, Li\\net al. (2024) apply the pooling operation to perform clustering:\\nˆs = Pool(s, k),\\n(5)\\nwhere Pool(·, ·) represents a pooling operation such as max(·) and Average(·), and k is the kernel size.\\nThis trick ensures that the identified tokens are continuous rather than isolated, resulting in more\\ncoherent semantics\\nB\\nHyper-parameters\\nIn this section, we introduce the hyper-parameter for reproduce.\\nWe first introduce our detected\\nevaluator heads and then introduce the hyper-parameter to use these heads.\\n14\\nDetected evaluator heads\\nWe conducted pilot experiments using the ”Needle-in-a-Haystack”\\nbenchmark across three popular LLMs: Llama-3.1-8B-Instruct, CodeLlama-7B, and Phi-3.5-mini-3.8B-Instruct.\\n• For Llama-3.1-8B-Instruct, which has 32 layers and 32 heads, the selected layer is 13, and the\\nchosen heads are [18, 13, 21, 8, 11, 1, 4, 3].\\n• For CodeLlama-7B, also with 32 layers and 32 heads, the selected layer is 14, and the selected\\nheads are [24, 3, 18, 7, 29, 2, 9, 1].\\n• Finally, for Phi-3.5-mini-3.8B-Instruct, which features 32 layers and 32 heads, the selected\\nlayer is 17, and the chosen heads are [7, 17, 30, 2, 6, 16, 25, 18].\\nUsing evaluator heads\\nThe hyperparameters applied during the evaluation of heads include the\\nsize of the observed windows, the pooling operation, and the kernel size for pooling. In all experiments,\\nwe used the average pooling operation, as the difference between average pooling and maximum pooling\\nwas negligible experimentally. For Llama-3.1-8B-Instruct, we set the size of the observed windows\\nand the kernel size for pooling to 16 and 32, respectively. For Phi-3.5-mini-3.8B-Instruct, the size\\nof the observed windows and the kernel size for pooling were set to 4 and 32, respectively. A larger\\nkernel size typically results in a more continuous compressed context, which is why we generally prefer\\nusing a larger kernel size.\\nC\\nComplexity\\nWe further discuss the complexity of NMI in this section. Consider an LLM f with L layers, H attention\\nheads per layer, and a hidden dimension d = dkH. Suppose the model f is given an input prompt of\\nN tokens, and generates t new tokens. In the pre-filling stage, each head computes Softmax( QKT\\n√dk )\\nwith Q, K ∈RN×dk, resulting in a complexity of O(dkN 2). Thus the total complexity for the pre-\\nfilling stage is O(LHdkN 2). During the decoding stage, the model generates t tokens based on the\\npre-cached K, V ∈RN×dk. At the i+1 tokens generated, the softmax operation in each head involved\\nQ ∈R1×dk, V ∈R(N+i)×dk and the complexity is O(dk(N + i)). to calculate the total complexity for\\ngenerating t tokens, we sum the complexities across all t tokens:\\nO(\\nt−1\\nX\\ni=1\\ndk(N + i)) = O(dk(Nt + (t −1)2\\n2\\n)) = O(dk(Nt + t2\\n2 )).\\n(6)\\nThus, the overall complexity for generating t tokens during the decoding stage is O(LHdk(Nt + t2)).\\nRegarding the complexity of EMI, it remains similar if the inference model is still transformer-based.\\nD\\nDatasets\\nWe present the synthetic and real-world benchmarks for assessing the capability of LLMs in handling\\nlong contexts.\\nD.1\\nSynthetic Benchmarks\\nNeedle-in-a-Haystack\\n(Kamradt, 2024) is a well-known synthetic dataset used to benchmark the\\nlong context ability. It involves randomly inserting a sentence into a variable-length long context,\\nfollowed by querying a given LLM to retrieve that specific sentence from long context.\\nRuler\\n(Hsieh et al., 2024) is a synthetic long-context benchmark that extends “Needle-in-a-Haystack”\\nby offering more complex tasks. RULER encompasses diverse types and quantities of needles and in-\\ntroduces new task categories, such as multi-hop tracing and aggregation, to evaluate behaviors beyond\\ndirect searching within the context.\\n15\\nD.2\\nReal-world Benchmarks\\nLongBench (Bai et al., 2024) is a widely used long-context benchmark that includes 21 datasets across\\nsix types of tasks. The six tasks are single-document question answering, multi-document question\\nanswering, summarization, few-shot learning, code completion, and synthetic tasks for retrieval and\\ncounting. In line with previous research (Jiang et al., 2023a,b), we focus on English datasets and\\nencompass six tasks across 16 datasets. We present the detailed introduction in the following context.\\nSingle-Doc QA\\n• NarrativeQA(Koˇcisk`y et al., 2018) is a standard question-answering dataset that includes texts\\nfrom Project Gutenberg and movie screenplays sourced from various websites. It contains 200\\nentries and is evaluated using the F1 metric.\\n• Qasper(Dasigi et al., 2021) is a question-answering dataset focused on NLP publications, fea-\\nturing abstractive, extractive, and yes/no questions. It consists of 200 entries and is evaluated\\nusing the F1 metric.\\n• MultiFieldQA-en (Bai et al., 2024) is created from diverse sources, including legal documents,\\ngovernment reports, encyclopedias, and academic publications. It includes 150 entries and is\\nevaluated using the F1 metric.\\nMulti-Doc QA\\n• HotpotQA(Yang et al., 2018) features many 2-hop questions crafted by native speakers, based\\non two related paragraphs. It contains 200 entries and is evaluated using the F1 metric.\\n• 2WikiMultihopQA(Ho et al., 2020) includes up to 5-hop questions systematically constructed\\nwith manual templates.\\nAnswering these questions requires reasoning paths that cannot be\\nresolved by local content alone. It contains 200 entries and is evaluated using the F1 metric.\\n• MuSiQue (Trivedi et al., 2022) consists of up to 4-hop questions, eliminating shortcuts and\\nquestions about naturalness. Each question includes 2-4 supplementary paragraphs that outline\\nthe reasoning path and relevant content. It comprises 200 entries and is evaluated using the F1\\nmetric.\\nSummarization\\n• GovReport(Huang et al., 2021) collects comprehensive reports containing human-written sum-\\nmaries from the U.S. Government Accountability Office and Congressional Research Service,\\ncovering a wide array of national policy issues. It includes 200 entries and is evaluated using the\\nRouge-L metric.\\n• QMSum(Zhong et al., 2021) contains annotated pairs of meeting summaries across various\\ndomains, including product, academic, and committee meetings. It includes 200 entries and is\\nevaluated using the Rouge-L metric.\\n• MultiNews (Fabbri et al., 2019) is a multi-document summarization dataset that clusters 2-10\\nnews articles discussing the same event or topic, each paired with a human-written summary,\\nthus forming a new long-text summarization task. It includes 200 entries and is evaluated using\\nthe Rouge-L metric.\\nFew-Shot Learning\\nTo construct few-shot learning with long text, (Bai et al., 2024) selected a\\nrange of training examples from the following datasets to concatenate the context in LongBench:\\n• TREC(Li and Roth, 2002) is a classification dataset featuring fine-grained class labels. It in-\\ncludes 200 entries and is evaluated using the accuracy metric.\\n• TriviaQA(Zhong et al., 2021) is another classification dataset that involves messenger-like con-\\nversations accompanied by human-written summaries. It contains 200 entries and is evaluated\\nusing the F1 metric.\\n16\\n• SAMSum (Fabbri et al., 2019) is a reading comprehension dataset consisting of question-answer\\npairs annotated with evidence passages. It includes 200 entries and is evaluated using the Rouge-\\nL metric.\\nCode Completion\\nCode completion is a critical yet challenging task utilized by auto-completion\\nsystems to assist users in predicting and completing code based on previous inputs and context.\\n• LCC(Guo et al., 2023): Sampled from the Long Code Completion dataset, this dataset is con-\\nstructed by filtering code based on length within individual GitHub files. It incorporates preced-\\ning lines of code as context, with the next line serving as the answer. This dataset includes 200\\nentries and is evaluated using the Exact Match (EM) metric.\\n• RepoBench-P(Liu et al., 2024a): Collected from GitHub repositories, this dataset aggregates\\nrelevant cross-file code snippets based on module import statements. These snippets are com-\\nbined with preceding lines of code in the current file to predict the next line of code, utilizing the\\nmost challenging XF-F setting. It comprises 200 entries and is evaluated using the Exact Match\\n(EM) metric.\\nSynthetic Task\\nTwo synthetic datasets evaluate the ability to retrieve and count from long contexts.\\n• PassageRetrieval-en: Derived from English Wikipedia, this dataset randomly samples 30 pas-\\nsages and selects one for summarization, with the task of identifying the original paragraph\\ncorresponding to the summary. It includes 500 entries and is evaluated using the Edit Similarity\\nmetric.\\n• PassageCount: This dataset presents a more complex challenge by randomly selecting para-\\ngraphs from English Wikipedia, repeating and shuffling them. The model is required to determine\\nthe number of unique passages among the provided set. It consists of 500 entries and is evaluated\\nusing the Edit Similarity metric.\\nZeroSCROLLS(Shaham et al., 2023) is a well-known long-context benchmark that encompasses\\nthree types of tasks: summarization, question answering, and aggregation across ten datasets. In line\\nwith prior research(Jiang et al., 2023a,b), we focus on the validation set for evaluation, as it is the only\\nset providing ground truth data. Below, we introduce the ten datasets across the three tasks, using\\nthe same evaluation metric for each dataset.\\nSummarization\\n• GovReport: Contains long reports from the Congressional Research Service and U.S. Govern-\\nment Accountability Office, paired with expert-written summaries.\\n• SummScreenFD: Comprises episode scripts from TV shows with community-contributed re-\\ncaps sourced from Wikipedia and TVMaze.\\n• QMSum: A query-based summarization dataset featuring meeting transcripts, including aca-\\ndemic, industrial, and parliamentary discussions, with each instance accompanied by a specific\\nquery.\\n• SQuALITY: A question-focused dataset derived from Project Gutenberg stories, requiring sum-\\nmaries based on crowdsourced guiding questions.\\nQuestion Answering\\n• Qasper: Contains NLP papers from the Semantic Scholar Open Research Corpus, with questions\\nbased on abstracts answered by practitioners.\\n• NarrativeQA: Features questions and answers derived from books and movie scripts, with\\nquestions crafted from summaries provided by annotators.\\n• QuALITY: Comprises stories and articles requiring multiple-choice questions that necessitate\\nreading substantial portions for accurate answers.\\n17\\n• MuSiQue: Focuses on multi-hop questions using Wikipedia paragraphs, including both answer-\\nable and unanswerable questions.\\nAggregation\\n• SpaceDigest: A sentiment aggregation task using 50 hotel reviews per hotel from the Space\\ndataset, focusing on strictly positive or negative reviews.\\n• BookSumSort: A task based on the BookSum dataset, requiring the reordering of shuffled\\nchapter summaries from selected books to their original order.\\nE\\nBaselines of prompt compression\\nSelective-Context\\napplies the logits of a causal language model to compute the self-information\\nfor each token, subsequently eliminating unnecessary content based on this self-information.\\nLLMLingua\\ndivides the target prompt into several segments and allocates different compression\\nbudgets according to the perplexity distribution of these segments.\\nLongLLMLingua\\nfurther incorporates task information (such as questions for document QA) and\\nemploys a Question-Aware strategy to enhance the density of key information in long contexts.\\nLLMLingua-2\\nis based on fine-tuned smaller BERT models to improve efficiency, leveraging global\\ninformation from an extractive text compression dataset annotated by ChatGPT.\\nF\\nAdditional Details for Evaluator Heads\\nIn this section, we describe how we construct task-aware synthetic data to identify task-aware heads.\\nWe focus on two types of tasks: multi-hop reasoning and code completion. For multi-hop reasoning,\\nwe utilize the multi-hop tracing task from the Ruler, which is an extension of “Needle-in-a-Haystack”.\\nMulti-hop tracing involves randomly inserting several interconnected chains to assess how effectively\\nthe model tracks all the content of these chains in response to given questions. This forms the basis of\\na multi-hop reasoning task. Regarding the code completion task, we first created a long code dataset\\nthat is unrelated to LongBench to prevent data leakage. We then manually inserted the code needing\\ncompletion into the context, thereby effectively generating the necessary evidence. These two types\\nof data are designed to identify specific heads within their respective domains using synthetic data\\ntailored to those domains.\\n18\\nG\\nExample Prompt Compression Results Using EHPC\\nOriginal Prompt\\nThey took hold of each other’s hands and wan-\\ndered out of the big Palace. They talked about\\ngrandmother, and about the roses upon the\\nroof.\\nWherever they went the winds lay still\\nand the sun broke through the clouds.\\nWhen\\nthey reached the bush with the red berries they\\nfound the reindeer waiting for them, and he\\nhad brought another young reindeer with him,\\nwhose udders were full. The children drank her\\nwarm milk and kissed her on the mouth. Then\\nthey carried Kay and Gerda, first to the Finn\\nwoman, in whose heated hut they warmed them-\\nselves and received directions about the home-\\nward journey. Then they went on to the Lapp\\nwoman; she had made new clothes for them and\\nprepared her sledge. Both the reindeer ran by\\ntheir side, to the boundaries of the country; here\\nthe first green buds appeared, and they said\\n’Good-bye’ to the reindeer and the Lapp woman.\\nThey heard the first little birds twittering and\\nsaw the buds in the forest. Out of it came riding\\na young girl on a beautiful horse, which Gerda\\nknew, for it had drawn the golden chariot. She\\nhad a scarlet cap on her head and pistols in her\\nbelt; it was the little robber girl, who was tired\\nof being at home. She was riding northwards to\\nsee how she liked it before she tried some other\\npart of the world. She knew them again, and\\nGerda recognised her with delight.\\nCompressed Prompt\\nThey took hold\\nwarm\\nmilk\\nthey\\ncarried Kay and Gerda, first to\\nFinn woman.\\nhomeward\\njourney\\nriding a young girl\\non a beautiful horse, which Gerda knew, for it\\nhad drawn the golden chariot. She had a scar-\\nlet cap on her head and pistols in her belt; it\\nwas the little robber girl, who was tired of be-\\ning at home. She was riding northwards to see\\nhow she liked it before\\nof the world. She knew them again, and Gerda\\nrecognised her with delight.\\n19\\n')\n"
     ]
    }
   ],
   "source": [
    "loader = ArxivLoader(query=\"2501.12959\", load_max_docs=1)\n",
    "docs = loader.load()\n",
    "pprint(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 文本分割\n",
    "（直接按字符串长度分割，也可使用token长度等）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=2)\n",
    "texts = spliter.split_documents(docs)\n",
    "# pprint(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 构建链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/c0q41fpx6l540lsl42_bzk5h0000gq/T/ipykernel_73163/2510094347.py:5: UserWarning: Parameters {'presence_penalty', 'top_p', 'frequency_penalty'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter.\n",
      "  llm = get_model('openai')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'本文提出了一种高效的、无需训练的提示压缩方法EHPC，通过评估头部在长文本输入中选择最重要的令牌，从而加速长文本推理。EHPC在两个主流基准测试中取得了最先进的结果，有效降低了商业API调用的复杂性和成本。与基于键值缓存的加速方法相比，EHPC具有竞争力，有望提高LLM在长文本任务中的效率。EHPC通过评估头部选择重要令牌，加速长文本推理，降低内存使用，并与KV缓存压缩方法竞争。EHPC在提示压缩基准测试上取得了新的最先进性能，降低了商业LLM的API成本和内存使用。'\n"
     ]
    }
   ],
   "source": [
    "doc_prompt = PromptTemplate.from_template(\"{page_content}\")\n",
    "#文本拼接\n",
    "content = lambda docs: \"\\n\\n\".join(doc.page_content for doc in docs)   \n",
    "prompt = PromptTemplate.from_template(\"请使用中文总结以下内容，控制在140个字以内：\\n\\n{content}\")\n",
    "llm = get_model('openai')\n",
    "\n",
    "# 链\n",
    "chain = (\n",
    "    {\"content\": lambda docs: content(docs)}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint(chain.invoke(texts[:50]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 链路检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+ \n",
      "| Parallel<content>Input | \n",
      "+------------------------+ \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "        +--------+         \n",
      "        | Lambda |         \n",
      "        +--------+         \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "      +------------+       \n",
      "      | ChatOpenAI |       \n",
      "      +------------+       \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +-----------------+    \n",
      "    | StrOutputParser |    \n",
      "    +-----------------+    \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pprint(chain.get_graph().print_ascii())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrenv3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
